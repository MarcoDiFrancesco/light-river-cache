{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "task = \"reg\"\n",
    "# task = \"clf\"\n",
    "\n",
    "if task == \"clf\":\n",
    "    task_name = \"Classification\"\n",
    "elif \"reg\":\n",
    "    task_name = \"Regression\"\n",
    "\n",
    "dpi=150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python vs Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_vs_rust(python_times, rust_times, task, task_name):\n",
    "    df_python = pd.DataFrame(python_times, columns=['Execution Time (sec)'])\n",
    "    df_rust = pd.DataFrame(rust_times, columns=['Execution Time (sec)'])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar('Python', df_python['Execution Time (sec)'].mean(), yerr=df_python['Execution Time (sec)'].std(), color='#6e6e6e', alpha=0.7, label='Execution Time (sec)')\n",
    "    plt.bar('Rust', df_rust['Execution Time (sec)'].mean(), yerr=df_rust['Execution Time (sec)'].std(), color='#ff000f', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Programming Languages')\n",
    "    plt.ylabel('Execution Time (sec)')\n",
    "    plt.title(f'Execution Time of Python vs Rust for {task_name}')\n",
    "    \n",
    "    plt.ylim(0, 15)\n",
    "\n",
    "    # Hide verical gray lines inside plot\n",
    "    plt.grid(axis='x', linestyle='')\n",
    "\n",
    "    # plt.legend(fontsize=legendsize)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'plots/python-vs-rust-{task}.png', dpi=dpi, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "# Provided times for Python and Rust\n",
    "python_times = [\n",
    "    11.77, 10.62, 10.00, 9.46, 10.90, 10.50, 8.48, 11.26, 9.98, 10.40,\n",
    "    9.18, 9.98, 9.16, 10.43, 9.31, 8.96, 12.19, 10.97, 9.15, 9.91,\n",
    "    9.77, 12.49, 11.29, 10.32, 14.11, 10.17, 9.60, 10.85, 11.47, 10.57,\n",
    "    10.34, 11.78, 9.52, 10.21, 10.53, 10.83, 11.63, 11.94, 10.05, 9.11\n",
    "]\n",
    "\n",
    "rust_times = [\n",
    "    2.845, 2.919, 2.835, 2.818, 3.067, 2.988, 3.084, 2.790, 2.818, 2.791,\n",
    "    2.807, 2.803, 2.797, 2.804, 2.780, 2.786, 2.807, 2.792, 2.791, 2.827,\n",
    "    2.971, 3.015, 2.960, 2.958, 2.835, 2.868, 2.816, 2.944, 2.845, 2.993,\n",
    "    2.840, 2.881, 2.833, 2.853, 2.935, 2.908, 2.920, 2.861, 2.900, 2.824\n",
    "]\n",
    "\n",
    "python_vs_rust(python_times, rust_times, \"clf\", \"Classification\")\n",
    "print(f\"Python: {np.array(python_times).mean():.2f}±{np.array(python_times).var():.2f}\")\n",
    "print(f\"Rust: {np.array(rust_times).mean():.2f}±{np.array(rust_times).var():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# Provided new times for Python and Rust\n",
    "python_times = [\n",
    "    13.02, 13.19, 12.64, 13.14, 12.93, 13.27, 13.46, 13.45, 14.24, 13.26,\n",
    "    13.58, 12.96, 13.44, 13.44, 13.38, 12.60, 13.33, 12.52, 13.01, 13.54,\n",
    "    13.09, 13.57, 13.33, 13.43, 13.82, 13.83, 12.94, 12.50, 13.21, 14.28,\n",
    "    13.35, 12.88, 13.52, 13.57, 14.24, 13.78, 13.65, 13.59, 13.38, 13.43\n",
    "]\n",
    "\n",
    "rust_times = [\n",
    "    0.456, 0.458, 0.462, 0.468, 0.465, 0.472, 0.463, 0.475, 0.475, 0.471,\n",
    "    0.480, 0.447, 0.467, 0.448, 0.471, 0.472, 0.507, 0.454, 0.460, 0.476,\n",
    "    0.476, 0.473, 0.485, 0.481, 0.463, 0.478, 0.529, 0.446, 0.459, 0.503,\n",
    "    0.473, 0.447, 0.450, 0.523, 0.483, 0.462, 0.470, 0.449, 0.483, 0.477\n",
    "]\n",
    "\n",
    "python_vs_rust(python_times, rust_times, \"reg\", \"Regression\")\n",
    "print(f\"Python: {np.array(python_times).mean():.2f}±{np.array(python_times).var():.2f}\")\n",
    "print(f\"Rust: {np.array(rust_times).mean():.2f}±{np.array(rust_times).var():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 50000\n",
    "\n",
    "fpath_notopt_times = f\"res_{task}_notopt_times_freq1000.csv\"\n",
    "fpath_opt_times = f\"res_{task}_opt_times_freq1000.csv\"\n",
    "df_no_opt = pd.read_csv(fpath_notopt_times)\n",
    "df_opt = pd.read_csv(fpath_opt_times)\n",
    "\n",
    "df_no_opt_head = df_no_opt.head()\n",
    "df_opt_head = df_opt.head()\n",
    "\n",
    "df_no_opt.columns = ['Inference Time', 'Train Time', 'Total Time']\n",
    "df_opt.columns = ['Inference Time', 'Train Time', 'Total Time']\n",
    "\n",
    "# Add the optimization labels\n",
    "df_no_opt['Optimization'] = 'Without Optimization'\n",
    "df_opt['Optimization'] = 'With Optimization'\n",
    "\n",
    "# Add block numbers to both datasets\n",
    "df_no_opt['Block'] = (df_no_opt.index // block_size) + 1\n",
    "df_opt['Block'] = (df_opt.index // block_size) + 1\n",
    "\n",
    "df = pd.concat([df_no_opt, df_opt])\n",
    "\n",
    "# Nano seconds to Micro seconds\n",
    "df['Inference Time'] = df['Inference Time'] / 1000\n",
    "df['Train Time'] = df['Train Time'] / 1000\n",
    "df['Total Time'] = df['Total Time'] / 1000\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_last_block_mean(df, task):\n",
    "    # Task: [\"Inference Time\", \"Train Time\", \"Total Time\"]\n",
    "    noopt = df[df[\"Optimization\"] == \"Without Optimization\"]\n",
    "    opt = df[df[\"Optimization\"] == \"With Optimization\"]\n",
    "\n",
    "    noopt = noopt.iloc[-block_size:-1]\n",
    "    opt = opt.iloc[-block_size:-1]\n",
    "\n",
    "    opt_mean = opt[task].median()\n",
    "    noopt_mean = noopt[task].median()\n",
    "    print(f\"Not optimized mean last block: {noopt_mean}\")\n",
    "    print(f\"Optimized mean last block: {opt_mean}\")\n",
    "    print(f\"Improvement: {100*(noopt_mean-opt_mean)/noopt_mean:.2f}%\")\n",
    "\n",
    "def print_last_block_inf_vs_train(df):\n",
    "    noopt = df[df[\"Optimization\"] == \"Without Optimization\"]\n",
    "    noopt = noopt.iloc[-block_size:-1]\n",
    "    inf_mean = noopt[\"Inference Time\"].median()\n",
    "    train_mean = noopt[\"Train Time\"].median()\n",
    "\n",
    "    print(f\"Inference (no optimization) mean last block: {inf_mean}\")\n",
    "    print(f\"Training (no optimization) mean last block: {train_mean}\")\n",
    "    print(f\"Inference takes {inf_mean/train_mean:.2f} more times than train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_last_block_mean(df, \"Total Time\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(\n",
    "    x=\"Block\",\n",
    "    y=\"Total Time\",\n",
    "    hue=\"Optimization\",\n",
    "    data=df,\n",
    "    linewidth=1,\n",
    "    width=0.5,\n",
    "    palette={\"With Optimization\": \"lightblue\", \"Without Optimization\": \"lightgreen\"},\n",
    "    showfliers=False,\n",
    ")\n",
    "plt.title(f\"Total execution time per iteration on {task_name} task\")\n",
    "plt.xlabel(\"Record Segments (Thousands)\")\n",
    "plt.ylabel(\"Time per iteration (µs)\")\n",
    "plt.xticks(\n",
    "    ticks=range(len(df[\"Block\"].unique())),\n",
    "    labels=[\n",
    "        f\"{int(i*block_size/1000)}-{int((i+1)*block_size/1000)}\"\n",
    "        for i in range(len(df[\"Block\"].unique()))\n",
    "    ],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.legend(title=\"\")\n",
    "# Hide verical gray lines inside plot\n",
    "plt.grid(axis=\"x\", linestyle=\"\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"plots/time-per-iter-tot-{task}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt = pd.melt(\n",
    "    df,\n",
    "    id_vars=[\"Block\", \"Optimization\"],\n",
    "    value_vars=[\"Inference Time\", \"Train Time\"],\n",
    "    var_name=\"Time Type\",\n",
    "    value_name=\"Time\",\n",
    ")\n",
    "df_melt[\"Category\"] = df_melt[\"Optimization\"] + \" - \" + df_melt[\"Time Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_last_block_inf_vs_train(df)\n",
    "\n",
    "df_melt_opt = df_melt[df_melt[\"Optimization\"] == \"With Optimization\"]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(\n",
    "    x=\"Block\",\n",
    "    y=\"Time\",\n",
    "    hue=\"Category\",\n",
    "    data=df_melt_opt,\n",
    "    showfliers=False,\n",
    "    linewidth=1,\n",
    "    width=0.5,\n",
    "    palette={\n",
    "        \"With Optimization - Inference Time\": \"lightseagreen\",\n",
    "        # \"Without Optimization - Inference Time\": \"paleturquoise\",\n",
    "        \"With Optimization - Train Time\": \"lightcoral\",\n",
    "        # \"Without Optimization - Train Time\": \"lightyellow\",\n",
    "    },\n",
    ")\n",
    "plt.title(f\"Train vs Inference execution time per iteration on {task_name} task\")\n",
    "plt.xlabel(\"Record Segments (Thousands)\")\n",
    "plt.ylabel(\"Time per iteration (µs)\")\n",
    "plt.xticks(\n",
    "    ticks=range(len(df[\"Block\"].unique())),\n",
    "    labels=[\n",
    "        f\"{int(i*block_size/1000)}-{int((i+1)*block_size/1000)}\"\n",
    "        for i in range(len(df[\"Block\"].unique()))\n",
    "    ],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.legend(title=\"\")\n",
    "# Hide verical gray lines inside plot\n",
    "plt.grid(axis=\"x\", linestyle=\"\")\n",
    "plt.grid(True)\n",
    "# plt.ylim(0, 70000)\n",
    "plt.savefig(f\"plots/time-per-iter-noopt-{task}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_last_block_mean(df, \"Train Time\")\n",
    "\n",
    "df_melt_train = df_melt[df_melt[\"Time Type\"] == \"Train Time\"]\n",
    "\n",
    "# Create the box plot with the specified categories\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.boxplot(\n",
    "    x=\"Block\",\n",
    "    y=\"Time\",\n",
    "    hue=\"Category\",\n",
    "    data=df_melt_train,\n",
    "    showfliers=False,\n",
    "    linewidth=1,\n",
    "    width=0.5,\n",
    "    palette={\n",
    "        \"With Optimization - Train Time\": \"lightcoral\",\n",
    "        \"Without Optimization - Train Time\": \"lightyellow\",\n",
    "    },\n",
    ")\n",
    "\n",
    "plt.title(f\"Train execution time per iteration on {task_name} task\")\n",
    "plt.xlabel(\"Record Segments (Thousands)\")\n",
    "plt.ylabel(\"Time per iteration (µs)\")\n",
    "plt.xticks(\n",
    "    ticks=range(len(df[\"Block\"].unique())),\n",
    "    labels=[\n",
    "        f\"{int(i*block_size/1000)}-{int((i+1)*block_size/1000)}\"\n",
    "        for i in range(len(df[\"Block\"].unique()))\n",
    "    ],\n",
    "    rotation=45,\n",
    ")\n",
    "\n",
    "plt.legend(title=\"Category\")\n",
    "\n",
    "# Hide verical gray lines inside plot\n",
    "plt.grid(axis=\"x\", linestyle=\"\")\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f\"plots/time-per-iter-train-{task}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_last_block_mean(df, \"Inference Time\")\n",
    "\n",
    "# Filter for Inference Time only\n",
    "df_melt_inf = df_melt[df_melt['Time Type'] == 'Inference Time']\n",
    "\n",
    "# Create the box plot with the specified categories\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.boxplot(x='Block', y='Time', hue='Category', data=df_melt_inf, showfliers=False, \n",
    "            linewidth=1, width=0.5,\n",
    "            palette={'With Optimization - Inference Time': 'lightseagreen', \n",
    "                     'Without Optimization - Inference Time': 'paleturquoise'})\n",
    "\n",
    "plt.title(f'Inference execution time per iteration on {task_name} task')\n",
    "plt.xlabel('Record Segments (Thousands)')\n",
    "plt.ylabel('Time per iteration (µs)')\n",
    "plt.xticks(ticks=range(len(df['Block'].unique())), \n",
    "           labels=[f\"{int(i*block_size/1000)}-{int((i+1)*block_size/1000)}\" for i in range(len(df['Block'].unique()))],\n",
    "           rotation=45)\n",
    "\n",
    "plt.legend(title='Category')\n",
    "\n",
    "# Hide verical gray lines inside plot\n",
    "plt.grid(axis='x', linestyle='')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.ylim(0, 140000)\n",
    "# plt.xlim(0, 12.5)\n",
    "\n",
    "plt.savefig(f'plots/time-per-iter-inf-{task}.png', dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_depth(freq):\n",
    "    fpath_notopt_depth = f\"res_{task}_notopt_depth_freq{freq}.csv\"\n",
    "    df = pd.read_csv(fpath_notopt_depth, header=None)\n",
    "\n",
    "    df.columns = ['Number of Nodes', 'Optimal Depth', 'Average Depth', 'Average Weighted Depth', 'Max Depth']\n",
    "\n",
    "    df['Number of Nodes'] = df['Number of Nodes'].astype(float) / 1000\n",
    "    df['Optimal Depth'] = df['Optimal Depth'].astype(float)\n",
    "    df['Average Depth'] = df['Average Depth'].astype(float)\n",
    "    df['Average Weighted Depth'] = df['Average Weighted Depth'].astype(float)\n",
    "    df['Max Depth'] = df['Max Depth'].astype(float)\n",
    "\n",
    "    df[\"Iteration\"] = df.index * freq / 1000\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depth = load_df_depth(100000)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_depth[\"Number of Nodes\"], df_depth[\"Iteration\"], color=\"purple\", marker=\"o\")\n",
    "plt.title(f\"Number of Nodes over Records for {task_name}\")\n",
    "plt.xlabel(\"Number of Nodes (thousands)\")\n",
    "plt.ylabel(\"Iterations (thousands)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"plots/node-count-{task}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "last_iter = df_depth.iloc[-1][\"Iteration\"]\n",
    "last_node_count = df_depth.iloc[-1][\"Number of Nodes\"]\n",
    "print(f\"Number of iterations: {last_iter}, Number of nodes: {last_node_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depth = load_df_depth(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "ax1.plot(\n",
    "    df_depth[\"Number of Nodes\"],\n",
    "    df_depth[\"Optimal Depth\"],\n",
    "    label=\"Optimal Depth (log2(#nodes))\",\n",
    "    # marker=\"o\",\n",
    ")\n",
    "ax1.plot(\n",
    "    df_depth[\"Number of Nodes\"],\n",
    "    df_depth[\"Average Depth\"],\n",
    "    label=\"Average Depth\",\n",
    "    # marker=\"o\",\n",
    ")\n",
    "ax1.plot(\n",
    "    df_depth[\"Number of Nodes\"],\n",
    "    df_depth[\"Max Depth\"],\n",
    "    label=\"Max Depth\",\n",
    "    color=\"orange\",\n",
    "    # marker=\"o\",\n",
    ")\n",
    "ax1.set_xlabel(\"Number of Nodes (thousands)\")\n",
    "ax1.set_ylabel(\"Depth\")\n",
    "ax1.set_title(f\"Depth metrics over Number of nodes for {task_name}\")\n",
    "ax1.grid(True)\n",
    "ax1.tick_params(axis=\"both\", which=\"major\")\n",
    "ax1.legend()\n",
    "plt.savefig(f\"plots/depths-{task}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Max depth: {df_depth['Max Depth'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "ax1.plot(\n",
    "    df_depth[\"Number of Nodes\"],\n",
    "    df_depth[\"Max Depth\"],\n",
    "    label=\"Max Depth\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "ax1.set_xlabel(\"Number of Nodes\")\n",
    "ax1.set_ylabel(\"Depth\")\n",
    "ax1.set_title(f\"Max and Average weighted depth compared for {task_name}\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.grid(True)\n",
    "ax1.tick_params(axis=\"both\", which=\"major\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    df_depth[\"Number of Nodes\"],\n",
    "    df_depth[\"Average Weighted Depth\"],\n",
    "    label=\"Average Weighted Depth\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "ax2.set_ylabel(\"Average Weighted Depth\", color=\"purple\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"purple\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# Add 10% padding to y-axis limits\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "ax1.set_ylim(0, y_max + 0.1 * (y_max - y_min))\n",
    "y2_min, y2_max = ax2.get_ylim()\n",
    "ax2.set_ylim(0, y2_max + 0.1 * (y2_max - y2_min))\n",
    "\n",
    "plt.savefig(f\"plots/depth-awd-{task}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_access(df, title, opt_str):\n",
    "    df.columns = [\"Sequential accesses\", \"Non-sequential accesses\"]\n",
    "    df.index = range(1, len(df) + 1)\n",
    "    df.index.name = \"Sample\"\n",
    "    df *= 100\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df.plot(kind=\"area\", stacked=True, color=[\"#ff000f\", \"#6e6e6e\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Iteration (thousands)\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.legend(title=\"Access Type\", loc=\"center right\")\n",
    "    plt.savefig(\n",
    "        f\"plots/seq-accesses-{opt_str}-{task}.png\", dpi=dpi, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_opt_sorted_count = f\"res_{task}_opt_sorted_count_freq1000.csv\"\n",
    "fpath_notopt_sorted_count = f\"res_{task}_notopt_sorted_count_freq1000.csv\"\n",
    "\n",
    "df_opt = pd.read_csv(fpath_opt_sorted_count)\n",
    "df_notopt = pd.read_csv(fpath_notopt_sorted_count)\n",
    "\n",
    "# Keep only 50k iterations\n",
    "df_opt = df_opt[df_opt.index < 50]\n",
    "df_notopt = df_notopt[df_notopt.index < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_access(df_opt, f\"Ratio of sequential accesses with optimization for {task_name}\", \"opt\")\n",
    "print(f\"Percentage of sequential accesses: {df_opt.iloc[-1]['Sequential accesses']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_access(df_notopt, f\"Ratio of sequential accesses without optimization for {task_name}\", \"nonopt\")\n",
    "print(f\"Percentage of sequential accesses: {df_notopt.iloc[-1]['Sequential accesses']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['number_of_nodes', 'iteration_number', 'time_taken_to_sort', 'train_inference_sum']\n",
    "fpath_notopt_sort_time = f\"res_{task}_notopt_sort_time_freq100000.csv\"\n",
    "fpath_opt_sort_time = f\"res_{task}_opt_sort_time_freq100000.csv\"\n",
    "df_not_optimized = pd.read_csv(fpath_notopt_sort_time, header=None)\n",
    "df_not_optimized.columns = columns\n",
    "df_optimized = pd.read_csv(fpath_opt_sort_time, header=None)\n",
    "df_optimized.columns = columns\n",
    "\n",
    "df_optimized['time_taken_to_sort_sec'] = df_optimized['time_taken_to_sort'] / 1e9\n",
    "df_combined = df_optimized.set_index(\"number_of_nodes\").join(df_not_optimized.set_index(\"number_of_nodes\"), lsuffix='_opt', rsuffix='_notopt')\n",
    "execution_time_gain = df_combined[\"train_inference_sum_notopt\"] - df_combined[\"train_inference_sum_opt\"]\n",
    "execution_time_gain = execution_time_gain / 1e9\n",
    "\n",
    "df_optimized[\"number_of_nodes\"] = df_optimized[\"number_of_nodes\"] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(\n",
    "    df_optimized[\"number_of_nodes\"],\n",
    "    df_optimized[\"time_taken_to_sort_sec\"],\n",
    "    label=\"Cost\",\n",
    "    marker=\"s\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "plt.xlabel(\"Number of Nodes (thousands)\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.title(f\"Time Cost over Size of the Tree for {task_name}\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\n",
    "    f\"plots/sorting-time-cost-{task}.png\", dpi=dpi, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data on the same axis\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(\n",
    "    df_combined.index / 1000,\n",
    "    execution_time_gain,\n",
    "    label=\"Gain\",\n",
    "    marker=\"o\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Nodes (thousands)\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.title(f\"Time Gain over Size of the Tree for {task_name}\")\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\n",
    "    f\"plots/sorting-time-gain-{task}.png\", dpi=dpi, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Robotic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "\n",
    "number_of_trees = list(range(1, 11))\n",
    "memory_footprint_mb = [\n",
    "    117977251, 254379320, 381582296, 508764584, 635225768,\n",
    "    763115536, 889746376, 1016018832, 1144669632, 1271891064\n",
    "]\n",
    "memory_footprint_mb = [x / (1024 ** 2) for x in memory_footprint_mb]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(number_of_trees, memory_footprint_mb)\n",
    "plt.title('Memory Footprint over Number of Trees for Regression')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Memory Footprint (MB)')\n",
    "plt.xticks(number_of_trees)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'plots/tree-count-vs-memory-reg.png', dpi=dpi, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "\n",
    "number_of_trees = list(range(1, 11))\n",
    "mse_values = [\n",
    "    0.12732017, 0.10974102, 0.10380474, 0.10095829, 0.099099696,\n",
    "    0.09776777, 0.09740372, 0.097054236, 0.09652473, 0.096195795\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(number_of_trees, mse_values)\n",
    "plt.title('MSE over Number of Trees for Regression')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.xticks(number_of_trees)\n",
    "plt.ylim(0.08, max(mse_values) + 0.01)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'plots/tree-count-vs-mse-reg.png', dpi=dpi, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_trees = list(range(1, 11))\n",
    "memory_footprint_mb = [\n",
    "    5407088,\n",
    "    11396704,\n",
    "    17181952,\n",
    "    23638064,\n",
    "    29309696,\n",
    "    36857288,\n",
    "    40638920,\n",
    "    45845832,\n",
    "    52426928,\n",
    "    58275096,\n",
    "]\n",
    "memory_footprint_mb = [x / (1024 ** 2) for x in memory_footprint_mb]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(number_of_trees, memory_footprint_mb)\n",
    "plt.title('Memory Footprint over Number of Trees for Classification')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Memory Footprint (MB)')\n",
    "plt.xticks(number_of_trees)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'plots/tree-count-vs-memory-clf.png', dpi=dpi, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "\n",
    "number_of_trees = list(range(1, 11))\n",
    "mse_values = [\n",
    "    0.88760096, 0.91904557, 0.9264443, 0.9316851, 0.93180835,\n",
    "    0.9337197, 0.9339047, 0.93495286, 0.9363709, 0.9363709\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(number_of_trees, mse_values)\n",
    "plt.title('Accuracy over Number of Trees for Classification')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.xticks(number_of_trees)\n",
    "plt.ylim(0.84, max(mse_values) + 0.01)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'plots/tree-count-vs-acc-clf.png', dpi=dpi, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "throughput = [2, 3.7, 5.2, 6.4, 7.4, 8.2, 8.8, 9.3, 9.7, 10]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(features, throughput, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "plt.ylabel(\"Throughput (MB/sec)\")\n",
    "plt.xlabel(\"# Features\")\n",
    "plt.xticks(features)\n",
    "plt.ylim(0, 11)\n",
    "plt.xlim(0, 21)\n",
    "plt.title(\"Relationship between Number of Features and Throughput\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Do not export the image: it's not added in directory \"6-results\" by in \"7-future-works\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
