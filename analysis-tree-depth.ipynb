{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# task = \"reg\"\n",
    "task = \"clf\"\n",
    "\n",
    "if task == \"clf\":\n",
    "    task_name = \"Classification\"\n",
    "elif \"reg\":\n",
    "    task_name = \"Regression\"\n",
    "\n",
    "fpath_notopt_times = f'res_{task}_notopt_times.csv'\n",
    "fpath_opt_times = f'res_{task}_opt_times.csv'\n",
    "\n",
    "fpath_notopt_depth = f'res_{task}_notopt_depth.csv'\n",
    "\n",
    "fpath_notopt_sorted_count = f'res_{task}_notopt_sorted_count.csv'\n",
    "fpath_opt_sorted_count = f'res_{task}_opt_sorted_count.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_caching = pd.read_csv(fpath_notopt_times)\n",
    "data_caching = pd.read_csv(fpath_opt_times)\n",
    "\n",
    "# Display the first few rows of each dataset to understand their structure\n",
    "data_no_caching_head = data_no_caching.head()\n",
    "data_caching_head = data_caching.head()\n",
    "\n",
    "data_no_caching_head, data_caching_head\n",
    "\n",
    "\n",
    "# Rename the columns for clarity\n",
    "data_no_caching.columns = ['Inference Time', 'Train Time', 'Total Time']\n",
    "data_caching.columns = ['Inference Time', 'Train Time', 'Total Time']\n",
    "\n",
    "# Add the optimization labels\n",
    "data_no_caching['Optimization'] = 'Without Optimization'\n",
    "data_caching['Optimization'] = 'With Optimization'\n",
    "\n",
    "# Add block numbers to both datasets\n",
    "block_size = 20000\n",
    "data_no_caching['Block'] = (data_no_caching.index // block_size) + 1\n",
    "data_caching['Block'] = (data_caching.index // block_size) + 1\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = pd.concat([data_no_caching, data_caching])\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a boxplot using seaborn to handle the coloring more effectively\n",
    "sns.boxplot(x='Block', y='Total Time', hue='Optimization', data=combined_data, \n",
    "            palette={'With Optimization': 'lightblue', 'Without Optimization': 'lightgreen'}, showfliers=False)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(f'Boxplot of execution time by iteration by Segments of 20 000 Records (With and Without Optimization) on {task_name} task')\n",
    "plt.xlabel('Record Segments')\n",
    "plt.ylabel('Total Time (ns)')\n",
    "plt.xticks(ticks=range(len(combined_data['Block'].unique())), \n",
    "           labels=[f\"{i*block_size}-{(i+1)*block_size}\" for i in range(len(combined_data['Block'].unique()))],\n",
    "           rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the data for plotting with specific labels\n",
    "melted_data = pd.melt(combined_data, id_vars=['Block', 'Optimization'], value_vars=['Inference Time', 'Train Time'], \n",
    "                      var_name='Time Type', value_name='Time')\n",
    "\n",
    "# Combine Optimization and Time Type for clearer labels in the plot\n",
    "melted_data['Category'] = melted_data['Optimization'] + ' - ' + melted_data['Time Type']\n",
    "\n",
    "# Create the box plot with the specified categories\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "sns.boxplot(x='Block', y='Time', hue='Category', data=melted_data, showfliers=False, \n",
    "            palette={'With Optimization - Inference Time': 'lightseagreen', \n",
    "                     'Without Optimization - Inference Time': 'paleturquoise', \n",
    "                     'With Optimization - Train Time': 'lightcoral', \n",
    "                     'Without Optimization - Train Time': 'lightyellow'\n",
    "                     })\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(f'Boxplot of Train and Inference Times by Segments of 20,000 Records (With and Without Optimization) on {task_name} task')\n",
    "plt.xlabel('Record Segments')\n",
    "plt.ylabel('Time (ns)')\n",
    "plt.xticks(ticks=range(len(melted_data['Block'].unique())), \n",
    "           labels=[f\"{i*block_size}-{(i+1)*block_size}\" for i in range(len(melted_data['Block'].unique()))],\n",
    "           rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Inference Time only\n",
    "inference_time_data = melted_data[melted_data['Time Type'] == 'Inference Time']\n",
    "\n",
    "# Create the box plot with the specified categories\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "sns.boxplot(x='Block', y='Time', hue='Category', data=inference_time_data, showfliers=False, \n",
    "            palette={'With Optimization - Inference Time': 'lightseagreen', \n",
    "                     'Without Optimization - Inference Time': 'paleturquoise'})\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(f'Boxplot of Inference Times by Segments of 20,000 Records (With and Without Optimization) on {task_name} task')\n",
    "plt.xlabel('Record Segments')\n",
    "plt.ylabel('Inference Time (ns)')\n",
    "plt.xticks(ticks=range(len(inference_time_data['Block'].unique())), \n",
    "           labels=[f\"{i*block_size}-{(i+1)*block_size}\" for i in range(len(inference_time_data['Block'].unique()))],\n",
    "           rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_data['Category'] = melted_data['Optimization'] + ' - ' + melted_data['Time Type']\n",
    "\n",
    "# Filter for Train Time only\n",
    "train_time_data = melted_data[melted_data['Time Type'] == 'Train Time']\n",
    "\n",
    "# Create the box plot with the specified categories\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "sns.boxplot(x='Block', y='Time', hue='Category', data=train_time_data, showfliers=False, \n",
    "            palette={'With Optimization - Train Time': 'lightcoral', \n",
    "                     'Without Optimization - Train Time': 'lightyellow'})\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(f'Boxplot of Train Times by Segments of 20,000 Records (With and Without Optimization) on {task_name} task')\n",
    "plt.xlabel('Record Segments')\n",
    "plt.ylabel('Train Time (ns)')\n",
    "plt.xticks(ticks=range(len(train_time_data['Block'].unique())), \n",
    "           labels=[f\"{i*block_size}-{(i*block_size+block_size)}\" for i in range(len(train_time_data['Block'].unique()))],\n",
    "           rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(fpath_notopt_depth, header=None)\n",
    "\n",
    "# Assuming the file has 5 columns, use only the first one\n",
    "nodes_count = data.iloc[:, 0]\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame({'Number of Records': range(1, len(nodes_count) + 1), 'Number of Nodes': nodes_count})\n",
    "\n",
    "# Multiply the number of records by 1000\n",
    "df['Number of Records'] = df['Number of Records'] * 1000\n",
    "\n",
    "# Plot the data again with the updated number of records\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Number of Records'], df['Number of Nodes'], color='purple')\n",
    "plt.title('Number of Nodes Over Records')\n",
    "plt.xlabel('Number of Records')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(fpath_notopt_depth, header=None)\n",
    "\n",
    "# Split the single column into four separate columns\n",
    "# data = data[0].str.split(',', expand=True)\n",
    "\n",
    "# Assign column names\n",
    "data.columns = ['Number of Nodes', 'Optimal Depth', 'Average Depth', 'Average Weighted Depth', 'Max Depth']\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "data['Number of Nodes'] = data['Number of Nodes'].astype(float)\n",
    "data['Optimal Depth'] = data['Optimal Depth'].astype(float)\n",
    "data['Average Depth'] = data['Average Depth'].astype(float)\n",
    "data['Average Weighted Depth'] = data['Average Weighted Depth'].astype(float)\n",
    "data['Max Depth'] = data['Max Depth'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data with secondary y-axis for Average Weighted Depth\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Optimal Depth, Average Depth, and Max Depth on the primary y-axis\n",
    "ax1.plot(data['Number of Nodes'], data['Optimal Depth'], label='Optimal Depth (log2(#nodes))')\n",
    "ax1.plot(data['Number of Nodes'], data['Average Depth'], label='Average Depth')\n",
    "ax1.plot(data['Number of Nodes'], data['Max Depth'], label='Max Depth', color='orange')\n",
    "\n",
    "# Set labels and title for the primary y-axis\n",
    "ax1.set_xlabel('Number of Nodes')\n",
    "ax1.set_ylabel('Depth')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data with secondary y-axis for Average Weighted Depth\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Optimal Depth, Average Depth, and Max Depth on the primary y-axis\n",
    "# ax1.plot(data['Number of Nodes'], data['Optimal Depth'], label='Optimal Depth (log2(#nodes))')\n",
    "# ax1.plot(data['Number of Nodes'], data['Average Depth'], label='Average Depth')\n",
    "ax1.plot(data['Number of Nodes'], data['Max Depth'], label='Max Depth', color='orange')\n",
    "\n",
    "# Set labels and title for the primary y-axis\n",
    "ax1.set_xlabel('Number of Nodes')\n",
    "ax1.set_ylabel('Depth')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Add 10% padding to y-axis limits\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "ax1.set_ylim(0, y_max + 0.1 * (y_max - y_min))\n",
    "\n",
    "# Create a secondary y-axis for Average Weighted Depth\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(data['Number of Nodes'], data['Average Weighted Depth'], label='Average Weighted Depth', color='purple')\n",
    "\n",
    "# Set label for the secondary y-axis\n",
    "ax2.set_ylabel('Average Weighted Depth', color='purple')\n",
    "ax2.tick_params(axis='y', labelcolor='purple')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Add 10% padding to secondary y-axis limits\n",
    "y2_min, y2_max = ax2.get_ylim()\n",
    "ax2.set_ylim(0, y2_max + 0.1 * (y2_max - y2_min))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_access(df, title):\n",
    "    # Rename columns\n",
    "    df.columns = ['Sequential accesses', 'Non-sequential accesses']\n",
    "\n",
    "    # Add an index for the number of samples\n",
    "    df.index = range(1, len(df) + 1)\n",
    "    df.index.name = 'Sample'\n",
    "\n",
    "    # Adjust the y-axis by multiplying by 100\n",
    "    df *= 100\n",
    "\n",
    "    # Create the area chart with the specified customizations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df.plot(kind='area', stacked=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Iteration (every 1000 samples)')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title='Access Type', loc='center right')\n",
    "\n",
    "    # Set custom ticks and labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(fpath_opt_sorted_count)\n",
    "sequential_access(df, \"Ratio of sequential accesses in a run. Run with Optimization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(fpath_notopt_sorted_count)\n",
    "sequential_access(df, \"Ratio of sequential accesses in a run. Run without Optimization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
